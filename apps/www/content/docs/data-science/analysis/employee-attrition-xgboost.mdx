---
title: XGBoost极端梯度提升
description: 机器学习实战系列：第七篇尝试XGBoost算法，通过贝叶斯优化探索复杂的参数空间，追求更高的预测精度
releaseDate: 2023-11-30
author:
  name: Richard Wang
  url: https://imrichard.com
---

逻辑回归拿到了0.88的测试集分数，随机森林反而掉到了0.87。随机森林的表现让人有点意外，但也说明了一个问题：这个数据集可能不需要特别复杂的非线性模式，或者说，数据量还不够大到让随机森林充分发挥优势。

XGBoost是另一个方向。它也是基于树的集成算法，但跟随机森林的思路不同。随机森林是"并行"的，每棵树独立训练，最后投票；XGBoost是"串行"的，每棵新树都在努力修正前面所有树的错误。这种Boosting策略在很多比赛中表现优异，理论上应该比随机森林更强。

但XGBoost的参数多得多，调起来也更费事。逻辑回归只有几个参数，随机森林稍微多一点，XGBoost有十几个重要参数需要考虑。这次我们还是用贝叶斯优化，但会跑更多轮次，给算法足够的时间探索参数空间。

## 参数的优先级

XGBoost的参数可以分成几类：控制学习速度的、控制树结构的、控制正则化的。不是每个参数的影响力都一样，调参时应该先关注那些影响最大的。

### 影响力最高的参数

**`n_estimators`（树的数量）**：决定了要训练多少棵树。树越多，模型学习能力越强，但训练时间也越长。和随机森林不同，XGBoost的树是串行训练的，所以这个参数对最终效果的影响很直接。通常从几十到几百都是合理范围。

**`learning_rate`（学习率，也叫eta）**：控制每棵树的贡献权重。学习率小，需要更多树才能达到好效果，但模型会更稳定；学习率大，收敛快，但容易过拟合。这是XGBoost最关键的参数之一，通常在0.01到0.3之间调整。

### 影响力较高的参数

**`max_depth`（树的最大深度）**：限制每棵树能长多深。XGBoost的默认值是6，比随机森林和GBDT都要保守。深度越大，单棵树的学习能力越强，但也更容易过拟合。这个参数在XGBoost中的影响力不如在随机森林中那么大，因为XGBoost还有很多其他机制来控制复杂度。

**`min_child_weight`**：子节点所需的最小样本权重和。这个参数越大，树越保守，分裂越少。它和树的深度一起控制着模型的复杂度。在小数据集上，这个参数的作用尤其明显。

**`subsample`**：每棵树训练时使用的样本比例。类似随机森林的bootstrap，但这里是显式控制。设置为0.8表示每棵树只用80%的数据训练，增加随机性，防止过拟合。

**`colsample_bytree`**：每棵树训练时使用的特征比例。这是XGBoost版本的`max_features`。和`subsample`一样，通过增加随机性来提高模型的泛化能力。

**`reg_alpha`和`reg_lambda`**：L1和L2正则化系数。XGBoost在损失函数中加入了正则项，这两个参数控制正则化的强度。`reg_lambda`（L2）是默认启用的，`reg_alpha`（L1）需要手动调整。正则化能让模型更简洁，减少过拟合。

### 其他参数

**`gamma`**：节点分裂所需的最小损失减少。这是另一个控制树复杂度的参数，但实际调参中，它的效果不如`min_child_weight`明显。

**`scale_pos_weight`**：正负样本的权重比。在类别不平衡的数据集上，可以设置为`负样本数/正样本数`，让模型更关注少数类。但这个参数需要和其他参数配合，单独调它不一定有用。

## 参数搜索策略

XGBoost的参数之间存在复杂的相互作用。比如学习率小了，就需要更多树；树的深度浅了，可能需要更多树来补偿；正则化强了，可能需要更多树或更大的学习率。这些关系很难人工推断，所以需要让优化算法去探索。

贝叶斯优化会根据前面试验的结果，推测哪些参数组合可能更好，然后优先去试那些区域。我们给它足够的预算（2500次试验），让它充分探索参数空间。

## 用 Optuna 调参

### 1. 数据预处理管道

这部分和之前一样，用的是特征工程后的数据集。

```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from xgboost import XGBClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
import optuna
from optuna.samplers import TPESampler

X_train = fe_train_df.drop('Attrition', axis=1)
y_train = fe_train_df['Attrition']
X_test = fe_test_df

# 识别分类变量和数值变量
fe_categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()
fe_numerical_cols = X_train.select_dtypes(exclude=['object']).columns.tolist()

# 创建预处理管道
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', fe_numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), fe_categorical_cols)
    ])
```

### 2. 定义目标函数

```python
def objective(trial):
    # 定义超参数搜索空间
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 10, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 7),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1),
        'subsample': trial.suggest_float('subsample', 0.2, 0.9),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.9),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 3),
    }

    # 创建 XGBoost 模型
    xgb = XGBClassifier(
        **params,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss'
    )

    # 创建包含预处理和模型的管道
    clf = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', xgb)
    ])

    # 进行交叉验证
    scores = cross_val_score(clf, X_train, y_train, cv=4, scoring='roc_auc', n_jobs=-1)
    return scores.mean()
```

这里的参数范围是根据经验设定的。`max_depth`限制在3到7之间，因为XGBoost的默认值就是6，太深了容易过拟合。`learning_rate`用对数均匀分布采样，因为它在0.01到0.1之间的效果差异很大。

交叉验证用4折而不是5折或10折，是为了加快速度。XGBoost的训练比逻辑回归慢得多，2500次试验已经需要跑很久了。

### 3. 运行优化器

```python
# 使用 Optuna 进行超参数优化
study = optuna.create_study(direction='maximize', sampler=TPESampler())
study.optimize(objective, n_trials=2500, n_jobs=-1)

# 输出最佳参数和最佳交叉验证得分
best_params = study.best_params
best_score = study.best_value
print("最佳参数:", best_params)
print("最佳交叉验证得分 (ROC AUC):", best_score)
```

**输出结果：**

```
最佳参数: {'n_estimators': 376, 'max_depth': 3, 'min_child_weight': 3, 'learning_rate': 0.02267965802969682, 'subsample': 0.5611217417898335, 'colsample_bytree': 0.2139570336695205, 'reg_alpha': 0.44010568075112483, 'reg_lambda': 0.23911084539970587}
最佳交叉验证得分 (ROC AUC): 0.8559756097560975
```

交叉验证分数是0.856，比逻辑回归的0.840和随机森林的0.842都要高。这个提升虽然不算特别大，但至少说明XGBoost确实学到了一些之前模型没捕捉到的模式。

### 4. 用最佳参数训练模型

```python
# 使用最佳参数训练模型
best_xgb = XGBClassifier(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    min_child_weight=best_params['min_child_weight'],
    learning_rate=best_params['learning_rate'],
    subsample=best_params['subsample'],
    colsample_bytree=best_params['colsample_bytree'],
    reg_alpha=best_params['reg_alpha'],
    reg_lambda=best_params['reg_lambda'],
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', best_xgb)
])

clf.fit(X_train, y_train)
```

## 提交到 Kaggle

用这个模型对测试集做预测，提交到Kaggle看看实际排名。

```python
# 对测试集进行预测
y_pred_test_proba = clf.predict_proba(X_test)[:, 1]

# 保存提交文件
submission = pd.DataFrame({
    'id': test_ids,
    'Attrition': y_pred_test_proba
})
submission.to_csv("xgb_submission.csv", index=False)
```

**提交结果：**

```
fileName               date                 description    status    publicScore  privateScore
---------------------  -------------------  -------------  --------  -----------  ------------
xgb_submission.csv     2023-04-22 22:23:53  xgb-best       complete  0.90071      0.88448
```

Private Score是0.88448，比逻辑回归的0.88385略高一点，比随机森林的0.87128高了不少。这是目前为止的最好成绩。

XGBoost的提升虽然不算特别大，但交叉验证分数从逻辑回归的0.84提到0.856，说明它确实学到了一些之前模型没捕捉到的模式。2500次试验跑了挺久，但贝叶斯优化找到的参数组合还算合理：学习率很小（0.023），所以需要更多树（376棵）；深度限制在3层，很保守；特征采样比例只有0.21，随机性很强。这些参数配合起来，让模型在复杂度和泛化之间找到了一个不错的平衡点。

和随机森林相比，XGBoost在测试集上的表现明显更好。这可能是因为XGBoost的正则化机制更精细，`reg_alpha`和`reg_lambda`能更好地控制模型复杂度，避免了随机森林那种"交叉验证好，测试集差"的过拟合问题。
