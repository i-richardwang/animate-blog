---
title: 理解文本向量化
description: 介绍文本向量化（词嵌入）技术：可视化 AI 如何用数字向量表示文本含义，计算语义相似度，以改进搜索和内容匹配的准确性
releaseDate: 2024-02-24
author:
  name: Richard Wang
  url: https://imrichard.com
---

随着人工智能（AI）在企业中的应用日益广泛，从智能客服到文档分析，从数据挖掘到知识管理，许多应用都依赖于让机器更有效处理人类语言、衡量内容间语义关联的**核心方法**。我们将探讨其中一项**关键手段**：**向量化（Vectorization）**，特别是**文本嵌入（Text Embedding）**。

这种**机制**是众多现代AI应用（如智能搜索引擎、个性化推荐系统、简历与岗位匹配、以及我们常提到的RAG技术）的重要组成部分。理解向量化，有助于我们了解AI是如何在关键词匹配之外，利用统计模式和分布式表示来**捕捉文本间的语义相似性**的。

## 传统搜索的局限：从字面到"意义"的鸿沟

想象一下，你需要在公司的海量文档库中查找关于"弹性工作制"的规定。

- **传统方法（关键词搜索）：**你可能会搜索"弹性工作制"、"灵活办公"等词语。系统会返回包含这些精确词语的文档。但如果某份关键文件使用了"非固定工时安排"或"混合办公模式细则"这样的表述，即使内容高度相关，传统搜索也会错过它们。
- **核心问题：**关键词搜索仅能匹配字面上的字符，无法有效捕捉词语背后的语义关联。

向量化技术正是为解决这个问题而生。它不仅关注文本表面的词语，更着眼于文本所表达的**深层语义信息**。

## 什么是向量化？为文本赋予数学表示

简单来说，向量化（Embedding）是将文本（可以是词语、句子、段落）转换成由**一串数字组成的"向量"**的过程。

你可以这样理解：我们使用经过特殊训练的 AI 模型（这些模型通过学习海量数据训练而成）为每个词语或文本片段在一个高维的**数学空间**（常被称为"语义空间"）中分配一个精确的"坐标"。这个"坐标"就是由一长串数字构成的向量。这个过程有两个关键特点：

1. **捕捉语义特征：** 这些数字（向量）并非随机生成，而是 AI 模型通过分析大量文本数据中的模式和上下文后得出的，能够**捕捉**文本的**关键语义特征**。
2. **衡量相似性：** 在这个"语义空间"中，意思相近的文本，它们的向量（坐标）距离也更近。这让计算机能够通过计算向量之间的距离来判断文本之间的语义相似度，即内容的关联程度。

这些概念可能听起来有些抽象，让我们通过几个例子来具体感受一下。

## 词语间的奇妙关系（国王 - 男人 + 女人 ≈ 女王？）

这是一个常被引用的例子，用来说明词向量如何能捕捉到词语间基于数据统计得出的某些关联关系。

为了进行这个演示，我们使用了像网易公司训练好的现成模型（`bce-embedding-base_v1`）来获取"国王"、"男人"、"女人"、"女王"这四个词语的向量。模型会为每个词语生成一个向量，即一长串数字。

以"国王"为例，它的向量可能包含上千个数字，这里仅展示开头和结尾的10个数字：

![国王一词的向量化结果](/ai/fundamentals/Example-of-Long-Word-Vector-Numbers.webp)

由于向量通常包含几百甚至上千个数字，直接查看数字列表并不直观。我们可以使用热力图来可视化：将每个数字用一个色块表示，数值越大颜色越深，数值越小颜色越浅。这样，一长串数字就转化为一行彩色条带，更便于观察模式。下面是"国王"这个词的向量热力图示例：

![国王一词向量的热力图可视化。每个色块代表向量中的一个数字，颜色深浅反映数值大小。](/ai/fundamentals/Heatmap-Representation-of-Word-King-Vector.webp)

了解了向量的可视化方法后，让我们进行一个有趣的计算：用"国王"的向量减去"男人"的向量，再加上"女人"的向量。这个操作相当于在向量层面模拟：从"国王"所代表的语义中移除与"男性"相关的特征，再添加与"女性"相关的特征。

理论上，这个计算结果的向量应该在语义上非常接近"女王"的向量。为了验证这一点，我们将计算结果向量和"女王"的向量用热力图表示出来，并排放在一起比较。

![词向量计算可视化。最下方两行分别代表"国王-男人+女人"的计算结果向量和"女王"的向量。](/ai/fundamentals/Semantic-Arithmetic-Vector-Visualization-Results.webp)

观察热力图中最后两行（即"国王-男人+女人"的计算结果和"女王"），我们可以**直观地看到它们的模式（颜色分布）高度相似**。这种视觉上的一致性证明，向量运算确实捕捉到了预期的语义变化，表明 Embedding 向量在其数值结构中编码了性别、地位等复杂的语义关系。

## 向量在二维和三维空间的可视化展示

除了能捕捉词语间的关系，Embedding 向量还有一个有趣的特性：**它们会自然地将意思相近的概念在"语义空间"中聚集在一起**。让我们换个角度来观察这一点。

我们选取了三个不同类别的词语作为示例：水果类（如"苹果"、"香蕉"），动物类（如"猫"、"狗"），以及建筑类（如"摩天大楼"、"寺庙"），并获取它们的 Embedding 向量。

这些向量维度很高，难以直观展示。为此，我们使用了一种数学降维方法（如 t-SNE 或 PCA，可以理解为一种"压缩"或"投影"技术），在降低维度的同时尽量保持向量之间的相对距离关系。这样，每个词语的向量就能在二维或三维空间中用一个点来表示。

![词汇向量在二维空间的表示。不同颜色代表不同类别（水果、动物、建筑）。](/ai/fundamentals/Two-Dimensional-Word-Vector-Clustering-Visualization.webp)

![词汇向量在三维空间的表示，可以更立体地观察聚类效果。](/ai/fundamentals/Three-Dimensional-Word-Vector-Clustering-Visualization.webp)

从图中可以清晰地看到**明显的聚类现象**：水果类词语自然聚在一起，动物类词语形成另一个群组，建筑类词语也各自成群。不同类别之间界限分明。这种基于语义的自然分组展示了 Embedding 的核心特征，让计算机能够**量化和识别**概念间的相似性，为语义搜索、信息分类和内容推荐等应用打下基础。

## 向量化驱动的智能信息检索

现在，让我们看看向量化技术在实际应用中如何发挥作用，特别是在**智能信息检索**领域。这项技术为企业知识库问答、法规文档查询、简历库人才搜索，以及相似历史项目案例匹配等提供了关键支持。以人力资源规章制度查询为例，让我们来拆解这个过程：

### 1. 知识库准备与向量化（准备阶段）：

首先是处理信息源。在 HR 场景中，这指的是公司的各项规章制度文档。系统会预先读取这些文档，并将每条规章（或根据需要将文档拆分成更小的段落）作为独立的文本单元。

![需要被向量化处理的规章制度文本片段](/ai/fundamentals/Sample-HR-Regulations-Text-Snippets.webp)

接着，系统调用 Embedding 模型，将这些文本片段转换成各自的向量。这些包含文本语义信息的向量与其对应的原文一起存入专门的"向量数据库"或"向量索引库"，为后续快速查找做准备。

### 2. 用户查询/输入向量化（使用时）：

当用户提出问题，如"我有多少天年假？"（或在其他场景下输入一份新简历、一个待匹配的案例描述等），系统会执行以下关键步骤：

**查询向量化：** 使用**与处理知识库时完全相同**的 Embedding 模型，将用户输入转换成一个查询向量。这确保了查询向量和文档向量处于同一个"语义空间"中，使得它们的比较具有实际意义。

### 3. 衡量语义相似性：找到"意义"上最近的邻居

有了用户查询向量和知识库中所有规章制度的向量后，关键问题是：如何判断哪些内容在**意思上最接近**用户的查询？

还记得**案例二**中，意思相近的词语（如各类水果）在降维后的空间中会自然聚集，它们的点**距离很近**吗？

这里运用的正是相同的原理！计算机需要计算出用户查询向量与知识库中每个文档向量之间的"**语义相似度**"。

- **核心思想：** 在"语义空间"中，向量间距离越近（或夹角越小），它们代表的语义就越相似。
- **计算方法：** 衡量这种"接近程度"最常用的是**余弦相似度（Cosine Similarity）**。无需深究数学细节，只需知道它会计算两个向量方向的接近程度，并得出一个相似度分数。
- **得分含义：** 得分越高（越接近 1），表示两个向量在方向上越一致，即代表的语义越相似，内容越相关。得分越低（越接近 -1 或 0，取决于向量表示方式），则表示语义越不相关甚至相反。

为了更直观地理解这个基于方向的相似性概念，我们可以看下面这个简化的三维空间示意图：

![3D空间向量相似度示意：比较向量(绿、红)与参考向量(蓝)的夹角大小，展示了方向越接近，余弦相似度越高的原理。](/ai/fundamentals/visualizing_cosine_similarity_3d_vectors.webp)

### 4. 执行语义检索与排序（查找匹配）：

系统使用余弦相似度，计算用户查询向量与向量库中**每个**规章制度向量的相似度得分。

### 5. 返回最相关结果：

系统根据相似度得分，对所有规章制度从高到低排序。得分最高的条目（即在"语义空间"中与查询向量最"接近"的文档）会作为最相关的结果呈现给用户。

下面的条形图展示了排序结果。当用户查询"我有多少天年假？什么情况下可以请假？"时，系统计算出的各规章制度与查询的余弦相似度得分如下：

![查询与各规章制度的余弦相似度排名。得分越高表示语义越相关。](/ai/fundamentals/Query-Document-Cosine-Similarity-Ranking-Chart.webp)

通过衡量向量间的语义相似度，系统准确识别出"休假规定"和"考勤制度"这两份文档与用户查询语义最接近，因此将它们排在最前。这种**基于语义相似度计算而非简单关键词匹配**的方式，是现代AI应用（如智能问答、简历筛选、案例推荐、内容发现等）能提供更智能、更准确服务的核心机制，标志着相比传统搜索方法的重大进步。

## 向量化的能力与边界

向量化技术的本质是把文本转换成数字向量，让机器能通过计算向量间的相似度来理解语义关联。从前面的案例可以看到，它能做很多事情：揭示词语间的复杂关系（国王-男人+女人≈女王）、让相似概念自动聚类（水果、动物、建筑各自成群），还能驱动智能搜索，找到在"意义"上最接近的文档。

这些能力推动 AI 应用从简单的"字符匹配"迈向更深层的"语义匹配"，是智能问答、简历筛选、内容推荐等应用的技术基础。

但需要明确的是，向量化擅长处理语义相似度和关联性任务，却不等同于 AI 具备了人类那样全面、灵活、带推理能力的语言"理解"。像大语言模型（LLMs）这样的系统，其处理语言的方式远比单纯的向量运算复杂得多，涉及注意力机制、Transformer 架构等多重技术的协同。

理解向量化的原理和边界，能帮我们更准确地认识这项技术的能力与局限，从而在实际应用中做出更合理的选择。如果你对 AI 在人力资源或其他领域的应用有想法，欢迎交流讨论。
