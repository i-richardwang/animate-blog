---
title: LLM训练各阶段数据类型与格式
description: 梳理LLM训练各阶段数据类型特点，覆盖分词、预训练、SFT阶段特点，并分析JSONL与Bin等格式选择考量
releaseDate: 2025-05-03
author:
  name: Richard Wang
  url: https://imrichard.com
image: https://imrichard.com/wp-content/uploads/2025/04/llm-data-prep-stages-and-formats-cover.avif
---

数据的准备是大型语言模型（LLM）训练流程中的一个基础环节。数据的质量和形态，很大程度上影响着模型最终能学习到的能力。LLM的训练并非单一过程，通常包含几个关键阶段，每个阶段对数据的侧重点和要求都不相同。同时，面对大规模数据，选择合适的文件存储格式对处理效率也有直接影响。

本文将梳理LLM训练中常见的几个阶段，探讨它们各自所需的数据类型、特点，并对比分析常用的数据存储格式，特别是JSONL和二进制格式。

## 不同训练阶段的数据需求

在前文《[大模型训练祛魅：从零构建流程详解](/docs/ai/fundamentals/guide-to-building-llms-from-scratch)》中，我们讲到了构建一个LLM通常涉及至少三个训练步骤：**分词器训练 (Tokenizer Training)**、**预训练 (Pre-training)** 和 **指令微调 (Supervised Fine-Tuning, SFT)**。每个步骤目标不同，所需的数据也有差异。

### 分词器训练数据：构建词汇表与分词规则

在模型学习语言之前，需要确定它处理文本的基本单元，即"词元"（Tokens）。分词器（Tokenizer）负责将原始文本切分成词元序列。这个过程侧重于**统计分析**，而非深度学习。

![图解LLM分词器训练数据：关键目标、数据特点与示例](https://imrichard.com/wp-content/uploads/2025/04/llm-tokenizer-training-data-infographic.avif)

- **目标**：建立一套合理的词汇表（Vocabulary）和有效的分词规则。
- **数据特点**：需要**大量且多样化的文本片段**，用以覆盖目标语言的常见字符和组合。单个文本片段不一定需要很长，干净、无错别字的文本是关键。
- **处理方式**：主要利用BPE、WordPiece等算法进行**统计分析**，识别高频模式，生成词表和规则，**不涉及损失函数优化**。
- **数据形态**：通常是纯文本集合。若使用JSONL格式，可能类似下图所示，每行一个包含文本的对象：

```json
{"text": "这是用于训练分词器的一段示例文字。"}
{"text": "Another example sentence for the tokenizer."}
```

### 预训练数据：奠定语言理解与知识基础

预训练是LLM获取广泛世界知识和基础语言能力的核心阶段。目标是让模型通过接触海量文本，掌握语言结构、事实知识、上下文理解等能力。

![图解LLM预训练数据：核心目标、海量数据特点与示例](https://imrichard.com/wp-content/uploads/2025/04/llm-pretraining-data-infographic.avif)

- **目标**：训练模型理解语言结构、预测文本序列，学习世界知识。
- **数据特点**：需要**极其庞大（TB级别或更高）且来源多样**的文本语料，如网页、书籍、代码等。数据**质量和其中蕴含的价值观**对模型影响深远。通常是**段落级别的连续文本**。
- **处理方式**：采用**自监督学习**。常见如**自回归语言建模 (Causal Language Modeling, CLM)**（预测下一个词元，如GPT）或**掩码语言建模 (Masked Language Modeling, MLM)**（预测被遮盖的词元，如BERT）。损失函数通常是交叉熵（针对预测的词元）。
- **数据形态**：较长的连续文本段落。JSONL格式示例如下：

```json
{
  "text": "大型语言模型的预训练阶段旨在通过接触海量的无标注文本数据，学习通用的语言表示。这个过程通常采用自监督学习范式，例如预测文本中的下一个词（Causal Language Modeling, CLM）或填充被掩盖的词（Masked Language Modeling, MLM）。例如，像GPT系列模型主要采用CLM目标，而BERT系列则以MLM闻名。预训练赋予了模型强大的泛化能力，使其能够理解和生成各种类型的文本。"
}
```

### 指令微调 (SFT) 数据：对齐人类意图

预训练后的模型知识丰富，但不一定擅长按指令完成任务或进行流畅对话。指令微调（SFT）旨在让模型学会理解并遵循人类的指示。

![图解LLM指令微调SFT数据：目标、高质量数据特点与示例](https://imrichard.com/wp-content/uploads/2025/04/llm-sft-instruction-tuning-data-infographic.avif)

- **目标**：使模型能够理解指令，并生成符合要求的、有帮助的回复。
- **数据特点**：需要**高质量的"指令-响应"配对**或**多轮对话数据**。数据量级通常远小于预训练，但对**质量要求非常高**，需要清晰地示范期望的交互模式。
- **处理方式**：属于**监督学习**。模型学习根据输入（指令/问题/上下文）生成接近目标答案的输出。常用的损失函数也是**交叉熵损失**，衡量生成序列与参考答案的匹配度。
- **数据形态**：结构化数据，通常包含指令、输入（可选）、输出（期望回复），有时还有对话历史。JSONL格式示例如下：

```json
{
  "instruction": "请解释一下什么是大型语言模型的预训练？",
  "input": "", // 有些场景下指令和输入分开，这里input为空
  "output": "大型语言模型的预训练是指在大规模无标注文本数据上训练模型，使其学习通用的语言知识和模式的过程。这个阶段的目标不是完成特定任务，而是建立一个强大的基础模型。",
  "history": [
    ["你好，我想了解一些关于LLM的信息。", "当然，请问您具体想了解哪方面呢？"]
  ]
}
```

## 理解常用数据格式

处理大规模数据时，文件格式的选择直接影响存储成本和读写效率。

### JSONL (JSON Lines)

JSONL是一种文本格式，特点是**每一行都是一个独立的、合法的JSON对象**。

```json
{"id": 1, "text": "Record one."}
{"id": 2, "text": "Record two.", "source": "web"}
```

![图解JSONL数据格式：核心定义、主要优缺点与格式示例](https://imrichard.com/wp-content/uploads/2025/04/llm-jsonl-data-format-infographic.avif)

**主要优点**:

- **易于流式处理**: 可逐行读写，适合大数据，内存占用低。
- **人类可读**: 便于检查、调试和理解数据内容。
- **灵活性**: 不同行的JSON结构可以不同（虽然实践中常保持一致）。
- **易于并行**: 文件可以方便地按行切分，分发给多个进程处理。

**主要缺点**:

- **存储效率不高**: 文本表示（如引号、逗号、字段名）占用了额外空间，文件体积相对较大。

### 二进制格式 (Binary Formats)

为了提高存储效率和加载速度，尤其在训练阶段需要反复读取数据时，常将预处理（如分词）后的数据转为**二进制格式**。这并非单一标准，而是泛指非文本的紧凑存储方式，例如直接存储Token ID序列。

![图解二进制数据格式：核心特点、主要优缺点与常见用途](https://imrichard.com/wp-content/uploads/2025/04/llm-binary-bin-data-format-infographic.avif)

**主要优点**:

- **高存储效率**: 无文本格式的冗余开销，显著减小文件体积。例如，几十GB的JSONL文本数据转换成Token ID序列后，体积可能缩小到几个GB。
- **潜在更快的加载速度**: 二进制数据通常能更快地读入内存并解析。

**主要缺点**:

- **人类不可读**: 无法直接查看内容，调试相对困难。
- **需要转换步骤**: 需额外流程将原始数据（如JSONL）转换成二进制，这本身有时间成本。
- **格式依赖特定实现**: 需要配套的代码来读取和解释数据结构。
- **常用于分词后**: 最适合存储数值型数据，如Token ID序列。

### 其他格式简介

- **CSV**: 简单表格数据适用，但处理嵌套结构或长文本不便，存储效率一般。
- **Parquet**: 列式存储，压缩和查询性能好，在数据分析场景常用，但对于LLM训练中常用的序列化文本或Token ID流，JSONL或Bin格式更直接。

### 格式选择的考量因素

- **数据规模与阶段**: 对TB级以上的预训练数据，转为Bin格式优化存储和IO是常见做法。对规模相对较小的SFT数据，JSONL的易用性可能更受青睐。
- **处理流程**: 若数据需多次中间处理和检查，JSONL的通用性和可读性有优势。若目标是最大化训练读取效率，且数据结构稳定（如已是Token ID），Bin格式效果更好。

## 数据准备的取舍

为大语言模型准备数据，本质上是个平衡各种需求的过程。分词器训练需要多样性，预训练追求规模和质量，微调则看重对齐度。不同阶段对数据的要求不同，数据格式的选择（JSONL 还是二进制）也要根据实际情况权衡。

如果你的数据量在 TB 级别，转成二进制格式能显著提升读取效率。但如果数据需要频繁调整和检查，JSONL 的可读性会让你省很多事。没有绝对的最优方案，关键是清楚自己的场景需要什么，能接受什么样的开发成本。

合理的数据策略，是模型训练效率和最终效果的基础。
