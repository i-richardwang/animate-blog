---
title: 绩效评价：结构化提取能力标签
description: 探索利用大语言模型（LLM）处理非结构化绩效评价文本，实现关键能力维度标签与评估依据的结构化提取
releaseDate: 2024-06-10
author:
  name: Richard Wang
  url: https://imrichard.com
image: https://imrichard.com/wp-content/uploads/2025/04/structured-performance-data-extraction-cover.avif
---

在人力资源管理实践中，上级撰写的员工绩效评价蕴含着丰富而有价值的信息。然而，这些包含大量细节和洞察的**非结构化文本**，长期以来**难以被系统性地分析和利用**。虽然我们意识到其潜在价值，却一直缺乏有效的技术手段来挖掘其中的深层信息，尤其是关于员工能力的具体反馈。

随着大语言模型（LLM）技术的发展，**提供了一种新的可能性**。通过 LLM 强大的自然语言理解和推理能力，我们是否能**有效处理**这些文本数据，从中提取出结构化的、有价值的见解？

本篇文章将分享一次**探索性的尝试**：我们运用 LLM，从绩效评价文本中**提取员工在关键能力维度的表现标签，并提供评价依据**。

## 实验设计

为了进行这次探索，我们设定了一个简化场景：分析主管的评价文本，评估员工在以下几个**预设能力维度**上的表现：

1. **业务与专业能力**
2. **协作与沟通能力**
3. **领导与管理能力**
4. **创新与适应能力**
5. **职业操守与自我管理能力**

我们期望模型能理解评价内容，为每个维度给出评级（"高"、"中"、"低"或"未知"），并从评价文本中**提供支持该评级的具体证据**。

<Callout type="warn">
本文**仅是一次技术可行性的初步验证**，使用的能力维度是**高度简化**的。在正式的员工评估中，我们强烈建议采用如 Hay Group、Aon Hewitt、Deloitte、北森等机构提供的成熟、专业的胜任力模型和工具。

演示所用的评价文本由 AI 生成，仅作示例。

</Callout>

### 核心思路：通过指令引导 LLM 进行推理与结构化输出

传统的关键词匹配等方法难以理解文本中微妙的语义和上下文。我们希望借助 LLM 的**深度理解能力**。关键在于如何有效地"指挥"LLM：

1. **明确角色与任务：** 指示模型扮演"人力资源专家"，执行能力评估。
2. **清晰的操作指引：** 将任务分解为具体步骤，包括理解文本、对照维度评估、给出明确评级（高/中/低/未知），并必须提供原文依据。
3. **规范输出格式：** 要求以 JSON 格式呈现结果，确保结构清晰，便于后续处理。

## 实践过程概览

<Callout type="info">
  接下来将介绍我们这次尝试的具体步骤。虽然会涉及一些技术实现的示例代码，**非技术背景的读者可以专注于了解每个步骤的目标和作用，无需深入研究代码细节。**
</Callout>

### 1. 设计引导性提示词（Prompt Engineering）

提示词是人与 LLM 沟通的桥梁。我们设计了一个模板，明确向 LLM 传达任务要求、执行步骤、评估维度、评级标准、理由要求和输出格式。

```python
# 提示词模板核心结构
evaluation_prompt = """
    作为人力资源专家，您的任务是分析绩效评价报告...

    1. 仔细阅读并理解评价文本...
    2. 根据文本内容，在[业务与专业能力、协作与沟通能力...]这些维度上，判断员工的表现级别为"高"、"中"、"低"或"未知"。
    3. 为你的每个判断提供理由：必须从评价原文中找到具体的描述作为依据...
    4. 请将结果以 JSON 格式输出，包含员工 ID 和各能力维度的评估（含"能力等级"和"评估理由"）...

    注意事项：
    - 评估必须基于提供的文本，避免主观推断。
    - ...

    员工ID与绩效评价 >>>{query}<<<
    \n{format_instructions}
    """
```

**为何强调"提供理由"？** 这不仅让结果可解释，更能验证模型是否真正理解了文本内容，而非凭空猜测。这是我们判断模型工作质量的重要依据。

### 2. 定义期望的输出结构

为了让模型输出的结果规范、一致，便于程序处理，我们使用了 Pydantic 定义了目标 JSON 格式。

```python
# Pydantic 模型定义
class PerformanceEvaluation(BaseModel):
    ID: str = Field(description="员工ID")
    业务与专业能力: dict = Field(description="包含`能力等级`和`评估理由`")
    协作与沟通能力: dict = Field(description="包含`能力等级`和`评估理由`")
    # ... 其他能力维度
```

_注：此步骤常结合 Langchain 等框架的 JsonOutputParser 来实现，它能帮助 LLM 更好地遵循格式要求，并自动解析输出。_

### 3. 构建自动化处理流程

我们将提示词、LLM 模型和输出解析器组合起来，形成一个处理管道（Chain）。输入员工 ID 和评价文本，理论上就能得到结构化的评估结果。

```python
# Langchain 处理链构建 (示意)
# prompt = PromptTemplate(...)
# parser = JsonOutputParser(...)
# chain = prompt | model | parser # 将组件串联

# 调用大模型获取结果
# chain_result = chain.invoke({"query": text})
```

## 效果观察

我们用一条示例数据进行测试：

**输入：**

```
{'ID': 'ID0001',
 'text': '1、成果显著、具备良好的价值导向、持续自我提升的动力。
 2、希望能持续成为团队的典范，激励并引领团队成员共同进步。
 3、在处理商业运营的复杂挑战和竞争策略探索方面表现出色。'}
```

**LLM 输出：**

```json
{
  "ID": "ID0001",
  "业务与专业能力": {
    "能力等级": "高",
    "评估理由": "评价内容提到员工在处理商业运营的复杂挑战和竞争策略探索方面表现出色，说明其业务专业能力很强，能够独立完成复杂的任务。"
  },
  "协作与沟通能力": {
    "能力等级": "高",
    "评估理由": "员工希望持续成为团队的典范，激励并引领团队成员，这表明他们在协作与沟通方面具有较高的能力，能够有效地与团队协作并影响他人。"
  },
  "领导与管理能力": {
    "能力等级": "未知",
    "评估理由": "虽然提到员工有激励团队的意愿，但评价内容中没有直接提及领导和管理的具体表现，因此这部分能力等级暂定为未知。"
  },
  "创新与适应能力": {
    "能力等级": "中",
    "评估理由": "评价中提到员工有自我提升的驱动力，但没有直接提及创新的具体例子，所以创新能力评级为中等。"
  },
  "职业操守与自我管理能力": {
    "能力等级": "高",
    "评估理由": "评价提到员工有良好的价值导向，这表明他们在职业操守方面表现出色，自我管理能力强。"
  }
}
```

从这个示例可以看出，模型能够按照指令从文本中提取相关信息，并给出评级和理由。值得注意的是，当某些维度的信息不足时，模型会审慎地评为"未知"并提供合理解释，这体现了其具备信息辨别能力。

## 思考与展望

这次小小的尝试初步展示了 LLM 在处理和理解非结构化 HR 文本方面的**潜力**。它为我们**分析和利用**过去难以处理的文本数据提供了一条新的可能路径。

当然，这还远不是一个成熟的解决方案，存在几个明显的局限：

1. **评估维度过于简化：** 尚未达到专业胜任力模型的深度和广度。
2. **结果依赖提示词工程：** 提示词质量直接决定输出效果。
3. **LLM 的稳定性与幻觉：** 模型可能产生不准确或虚构的信息，需要严格的验证机制。
4. **原始数据质量：** 评价文本本身的清晰度和具体性极大影响分析结果。

**后续可能的优化方向：**

- **任务分解：** 让模型每次聚焦评估单一能力维度，并提供具体判断标准和案例（Few-shot Learning），以提高准确性。
- **结果验证与迭代：** 结合人工审核验证模型输出，利用评估理由进行诊断，持续优化提示词和流程。
- **融入专业知识：** 将成熟的胜任力模型定义和行为锚点融入提示词设计，引导模型做出专业判断。

总而言之，利用大语言模型从绩效评价文本中提取结构化信息，是一个**值得继续探索的方向**。虽然目前展示的只是一个非常初步的尝试，但它提示了利用新技术处理传统 HR 数据的潜在价值。要将其真正应用于实践，**还需要大量的验证工作和持续优化，并必须结合人力资源的专业知识和经验**把关，以确保结果的可靠性和有效性。
