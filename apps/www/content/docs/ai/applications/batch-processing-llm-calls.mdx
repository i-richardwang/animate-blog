---
title: 批量调用提升大模型任务效率
description: 以多语言文本翻译为例，展示如何通过批量调用显著提升大模型处理效率并降低成本，对比单轮循环与批量处理的性能差异
releaseDate: 2024-06-08
author:
  name: Richard Wang
  url: https://imrichard.com
---

面对大规模文本数据时，逐条调用大模型会带来巨大的时间成本。假设需要处理一个包含数百条多语言员工反馈的数据集，如果通过聊天界面或单条循环调用的方式处理，可能需要耗费数小时。

批量调用提供了一个更高效且更经济的解决方案。通过将多个请求打包成批次并行处理，不仅可以显著缩短总体处理时间，还能大幅降低 API 调用成本。许多在线大模型服务商（如 OpenAI、Anthropic 等）都为批量 API 提供了价格优惠，通常能获得 50% 的折扣。这里以文本翻译任务为例，展示批量调用的具体实现和效果对比。

<Callout type="info">
  大模型在文本翻译任务中的优势在于其强大的上下文理解能力和语言生成能力，能够提供更准确的用词和更流畅的表达，特别是处理专业术语或特定领域内容时。
</Callout>

## 准备多语言数据集

我们构建了一个包含 200 条虚构员工调研反馈的多语言数据集，用于模拟国际化企业的真实场景：

- 100 条英文反馈
- 西班牙语、葡萄牙语、法语、德语和日语各 20 条

数据集样本：

| id | feedback_text                                                                                                                               |
|----|---------------------------------------------------------------------------------------------------------------------------------------------|
| 1  | A liderança em nossa empresa é de primeira linha. Os gerentes são acessíveis e fornecem grande suporte.                                     |
| 2  | Estoy muy impresionado con la visión de la empresa para el futuro. El equipo de liderazgo ha establecido una dirección clara e inspiradora. |
| 3  | My career development here has been satisfactory. I have had opportunities to learn new skills and take on new challenges.                  |
| 4  | O equilíbrio entre trabalho e vida é um pouco desafiador com a carga de trabalho atual. Horários mais flexíveis seriam apreciados.          |
| 5  | 福利厚生は良いですが、給与は業界標準と比較してもっと競争力があっても良いと感じます。                                                                                                  |

## 构建翻译任务链

### 定义数据模型

使用 Pydantic 定义翻译结果的数据结构，这样可以确保输出格式的一致性和类型安全：

```python
from pydantic import BaseModel, Field

class TranslatedText(BaseModel):
    """翻译后的文本"""
    translated_text: str = Field(..., description="翻译成中文的文本内容")
```

### 设计提示词

系统提示和用户提示的设计直接影响翻译质量。系统提示定义翻译的基本规则和要求，用户提示提供具体的待翻译文本：

```python
system_message = """
你是一位精通多语言的翻译专家。你的任务是将给定的{text_topic}文本准确翻译成中文。请遵循以下指南：

1. 翻译要求：
   - 仔细阅读每条反馈，理解其核心内容和语境。
   - 将反馈准确翻译成中文，保持原意不变。
   - 确保翻译后的文本通顺、自然，符合中文表达习惯。
   - 如遇专业术语或特定概念，请尽可能找到恰当的中文对应表述。

2. 输出格式：
   - 对每条反馈，输出对应的中文翻译。
   - 忽略原始文本中的特殊格式，按照一段话的形式输出翻译结果，不要包含特殊字符。

请确保翻译的准确性和一致性，不要遗漏任何反馈。
"""

human_message_template = """
请将以下{text_topic}文本翻译成中文。

===
{feedback_text}
===

请按照系统消息中的指南进行翻译，并以指定的JSON格式输出结果，但不要在输出中重复json schema。
"""
```

### 创建任务链

使用自定义的 Chain 类组装翻译任务链：

```python
translate_feedback = CustomChain(
    TranslatedText, 
    system_message, 
    human_message_template, 
    language_model
)()
```

这个任务链整合了数据模型、提示词模板和语言模型，形成一个可复用的翻译工具。

<Callout type="info">
  关于 `CustomChain` 的具体实现，可以参考工程化任务函数的相关文档。这个类封装了 LangChain 的常用模式，简化了结构化输出任务的构建流程。
</Callout>

## 实现批量处理函数

为了充分利用大模型的并行处理能力，我们实现了一个通用的批量处理函数。这个函数不仅适用于翻译任务，也可以用于其他类型的批量文本处理场景：

```python
import pandas as pd
from tqdm import tqdm
import traceback

def process_dataset_batch(llm_chain, data, field_mapping, fixed_params, 
                          additional_fields=None, batch_size=10):
    """
    处理数据集，调用大模型任务链，并返回处理结果和错误信息。
    
    参数说明：
    - llm_chain: 大模型任务链
    - data: 数据集（pandas.DataFrame）
    - field_mapping: 字段映射（dict，键为invoke参数名，值为数据集字段名）
    - fixed_params: 固定参数（dict）
    - additional_fields: 需要额外保存的字段（list，默认为None）
    - batch_size: 批处理大小（int，默认为10）
    
    返回值：
    - result_df: 处理结果的DataFrame
    - error_df: 错误信息的DataFrame
    """
    results = []
    errors = []
    
    for start in tqdm(range(0, data.shape[0], batch_size), desc="执行进度"):
        end = min(start + batch_size, data.shape[0])
        batch_data = data.iloc[start:end]
        
        # 为当前批次构建调用参数
        batch_invoke_params = []
        for index, row in batch_data.iterrows():
            invoke_params = {
                invoke_field: row[data_field] 
                for invoke_field, data_field in field_mapping.items()
            }
            invoke_params.update(fixed_params)
            batch_invoke_params.append(invoke_params)
        
        try:
            # 批量调用大模型
            responses = llm_chain.batch(batch_invoke_params)
            
            # 处理响应结果
            for i, response in enumerate(responses):
                result = {}
                if additional_fields:
                    for field in additional_fields:
                        result[field] = batch_data.iloc[i][field]
                
                if isinstance(response, list):
                    for item in response:
                        result.update(item)
                else:
                    result.update(response)
                
                results.append(result)
        
        except Exception as e:
            # 记录错误信息
            for i in range(len(batch_data)):
                error_info = {"index": start + i}
                if additional_fields:
                    for field in additional_fields:
                        error_info[field] = batch_data.iloc[i][field]
                error_info["error"] = str(e)
                errors.append(error_info)
            
            print(f"Error processing batch {start}-{end}: {e}")
            traceback.print_exc()
    
    result_df = pd.DataFrame(results)
    error_df = pd.DataFrame(errors)
    
    return result_df, error_df
```

这个函数的核心机制：

1. 将数据集分割成指定大小的批次
2. 为每个批次构建调用参数
3. 批量调用大模型任务链
4. 处理响应结果并记录可能的错误
5. 返回处理结果和错误信息的 DataFrame

## 性能对比

为了直观展示批量调用的效率提升，我们对比了单轮循环和批量调用两种方式处理相同数据集的表现。

### 单轮循环

设置批处理大小为 1，模拟逐条调用的情况：

```python
field_mapping = {'feedback_text': 'feedback_text'}
fixed_params = {'text_topic': '员工调研反馈'}

result_df, error_df = process_dataset_batch(
    translate_feedback, 
    raw_feedback_data, 
    field_mapping,
    fixed_params=fixed_params,
    additional_fields=['id', 'feedback_text'],
    batch_size=1
)
```

输出结果：

```text title="单轮循环"
执行进度: 100%|██████████| 200/200 [02:00<00:00,  1.66it/s]
```

### 批量调用

设置批处理大小为 4，实现批量处理：

```python
field_mapping = {'feedback_text': 'feedback_text'}
fixed_params = {'text_topic': '员工调研反馈'}

result_df, error_df = process_dataset_batch(
    translate_feedback, 
    raw_feedback_data, 
    field_mapping,
    fixed_params=fixed_params,
    additional_fields=['id', 'feedback_text'],
    batch_size=4
)
```

输出结果：

```text title="批量调用"
执行进度: 100%|██████████| 50/50 [01:06<00:00,  1.34s/it]
```

### 效果分析

单轮循环处理 200 条数据用时约 120 秒，批量调用（批大小为 4）处理相同数据集用时约 66 秒，**效率提升了约 45%**。

这个提升幅度在翻译这类相对简单的任务上已经很明显。对于更复杂的任务（如文本分析、内容生成、结构化数据提取等），批量调用的优势会更加显著。

<Callout type="warn">
  `batch_size` 的选择需要根据具体任务和硬件资源调整。较大的批处理大小能更好地利用硬件资源，但也可能导致内存占用过高或处理速度下降。对于自部署的大模型服务，还需要考虑单次请求的 payload 限制和超时设置。
</Callout>

## 批量处理的实践建议

从这个翻译任务的实践中，有几个值得注意的点：

**成本优势明显。** 使用在线大模型服务时，批量 API 通常能获得显著的价格优惠。OpenAI 的 Batch API 提供 50% 的折扣，其他服务商（如 Anthropic、国内的字节跳动、阿里巴巴等）也有类似的优惠政策。代价是批量任务的响应时间可能需要等待数小时到 24 小时，但对于非实时场景（如离线数据处理、定期报告生成等），这个延迟完全可以接受。以处理 10 万条文本为例，使用批量 API 可能节省数百到上千元的成本。

**错误处理很重要。** 批量调用时，某一批次的失败不应影响其他批次。上面的函数通过 try-except 捕获每个批次的错误，确保整个流程能继续执行，同时记录详细的错误信息供后续排查。

**批次大小的设置策略。** `batch_size` 的选择取决于使用的是自部署服务还是在线 API：

- **自部署大模型**：主要考虑硬件资源（显存、CPU 等）。建议从较小的值（如 4 或 8）开始测试，逐步增加直到找到性能和稳定性的平衡点。过大的批次可能导致显存溢出或处理速度下降。
- **在线大模型 API**：受服务商的速率限制约束（如 RPM - 每分钟请求数、TPM - 每分钟 Token 数）。这时 `batch_size` 应该设置在不触发限流的范围内，具体数值取决于你的 API 配额和并发限制。

需要注意的是，这里的 `batch_size` 是指客户端代码中将数据分批处理的大小（并发调用优化），与在线服务商提供的 Batch API（如 OpenAI Batch API）是两个不同层面的概念。Batch API 是服务商层面的异步批处理接口。

**保留原始数据。** 通过 `additional_fields` 参数保留原始文本和 ID，方便后续校验结果或排查问题。这在生产环境中特别有用。

**监控进度和错误。** 使用 tqdm 显示进度条，及时发现处理异常。如果错误率较高，可能需要调整提示词或检查数据质量。

批量调用不是万能的，但它是处理大规模文本数据时提升效率和降低成本的有效手段。关键是要根据具体场景找到合适的批次大小，并做好错误处理和结果校验。如果使用在线服务且对时效性要求不高，可以考虑直接使用服务商提供的 Batch API 来获得更大的成本优势。

