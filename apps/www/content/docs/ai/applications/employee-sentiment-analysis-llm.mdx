---
title: 利用大模型提升情感分析准确性
description: 利用Qwen-14B大模型进行员工调研情感分析，初步测试验证其在处理反语、混合情感文本上的显著优势
releaseDate: 2024-01-28
author:
  name: Richard Wang
  url: https://imrichard.com
image: https://imrichard.com/wp-content/uploads/2025/04/enhanced-employee-sentiment-analysis-with-llms-cover.avif
---

在员工调研中准确分类回复文本的情感倾向是一项重要的基础工作。传统的情感分析方法在处理反语、讽刺，以及同时包含正面和负面表述的长文本等复杂情况时,效果往往不尽理想。大语言模型（LLM）的发展为解决这些挑战开辟了新的方向。

为此，我们开展了一项初步测试，使用本地部署的 Qwen-14B 模型处理这类情感分类任务，并与传统的 NLP 方法进行对比。测试结果表明，大模型在处理此类文本时展现出显著潜力。以下我们将分享这次测试的过程与初步结论。

## 任务背景

我们的目标是对员工调研中的主观回复文本进行情感倾向分类，具体分为 **正向**、**中性**、**负向** 三类。这项任务的难点在于，员工的表达方式多样，有时并非直抒胸臆：

1. **反话与讽刺：** 字面意思与真实意图可能完全相反。
2. **混合情感：** 一段回复中可能同时包含肯定和抱怨。
3. **语境依赖：** 部分表达需要结合上下文才能准确判断情绪。

传统的NLP方法（如基于词典、关键词或一些早期机器学习模型）在捕捉这种深层语义和语境细微差别方面能力有限，容易导致误判。

## 实验设计

考虑到大模型在自然语言理解方面的优势，我们尝试利用其强大的语义捕捉能力来应对上述挑战。我们选择了 Qwen-14B 模型，并在本地进行部署，以确保数据安全和响应效率。

### 1. 数据集构建

为了快速验证想法，我们构建了一个小型的虚构数据集，包含20条模拟的员工回复。这些回复由人工编写并辅以ChatGPT辅助生成，力求覆盖一些简单、复杂及具有挑战性的文本类型。

这是一个规模非常小且非真实的测试集，其主要目的是初步观察模型表现，测试结果的普适性有限。

### 2. 评分规则

情感判断本身存在一定主观性，尤其是在"中性"与"正向"、"中性"与"负向"的边界上，不同标注者也可能存在分歧。为了更客观地评估模型效果，我们采用了差异化的赋分制：

- **完全一致：** 模型判断与人工标注一致，得 1 分。
- **轻微偏差：** 模型将"中性"判断为"正向"或"负向"（反之亦然），得 0.5 分。
- **完全相反：** 模型将"正向"判断为"负向"（反之亦然），得 0 分。

这种评分方式更能体现模型在理解模糊地带和避免严重错误上的能力。

### 3. 技术实现简述

我们使用 LangChain 框架来组织与大模型的交互。关键步骤包括：

- **定义输出格式：** 要求模型返回包含文本ID和情感分类结果（"正向"、"中性"或"负向"）的结构化数据（JSON），便于后续处理。
- **设计提示词 (Prompt Engineering)：** 这是引导大模型完成任务的关键。我们设计的提示词主要包含以下指令：
  - 明确角色：扮演NLP专家，评估员工回复。
  - 清晰任务：进行情感分类（正向/中性/负向），特别注意处理反话和讽刺，识别真实意图。
  - 约束条件：基于原文判断，避免推断；输出结果必须是三类标签之一。
  - **实践发现：** 在提示词中直接要求模型输出中文标签（"正向"、"中性"、"负向"）比输出英文标签（"positive", "negative", "neutral"）得到的准确性更高。这提示我们在与大模型交互时，指令的具体措辞和语言选择也可能影响效果。
- **构建任务链：** 将提示词模板、模型实例和输出解析器组合成一个可执行的任务链。
- **批量处理与结果合并：** 遍历数据集中的每条回复，调用任务链进行情感分类，并将结果与原始数据合并。

如果你对实现代码感兴趣，可以参考如下代码来实现：

```python
# 定义输出格式
response_schemas = [
ResponseSchema(name="ID", description="ID"),
ResponseSchema(name="sentiment_class_llm", description="大模型的情感分类，返回`正向`、`中性`或`负向`"),
]

output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()

# 提示词
classification_prompt = """
作为一个NLP专家，你需要评估员工敬业度调研中的回复内容。请按照以下步骤操作：

    1. 情感分类：根据回复内容，将情绪归类为"正向"、"中性"或"负向"。注意回复的情感色彩、态度和情绪。对于使用反话或反讽的回复，尝试识别实际意图，并据此分类。
    2. 请基于提供的回复内容做出判断，避免任何推测或脑补。

    要点提醒：
    - 直接回答每个任务的问题。
    - 确保情感分类结果仅为"正向"、"中性"或"负向"之一。

    员工ID与员工回复 >>>{answer}<<<
    \n{format_instructions}
    """

# 创建提示模板
classification_prompt = PromptTemplate(
input_variables=['answer'],
template=classification_prompt,
partial_variables={"format_instructions": format_instructions}
)

# 创建LLMChain
classification_chain = LLMChain(
llm=llm,
prompt=classification_prompt
)
```

### 4. 评分计算

在得到模型输出后，我们根据预设的编码（正向:0, 中性:1, 负向:2）计算模型预测结果与人工标注（Ground Truth）之间的得分差异，并依据评分规则计算每条样本的得分。最终计算所有样本的平均分，并统计正负向判断完全相反的错误占比。

## 测试结果与分析

经过对 20 条样本的测试，我们根据前述评分规则计算了两种方法的得分情况：

| **模型** | **平均得分** | **识别错误占比** |
| -------- | ------------ | ---------------- |
| 大模型   | 0.950        | 0%               |
| 传统 NLP | 0.775        | 15%              |

**结果解读：**

- **整体表现：** 在这个小规模测试集上，**大模型的平均得分（0.95）明显高于传统 NLP 方法（0.775）**。
- **关键优势：** 更值得注意的是，大模型**没有出现将正向情感判断为负向（或反之）的严重错误**（识别错误占比 0%），而传统 NLP 方法出现了 15% 的此类错误。这意味着大模型在理解文本的深层语义和复杂情感表达方面可能具有显著优势，尤其是在避免方向性误判上。

这初步表明，大模型凭借其更强的上下文理解能力和对语言细微差别的捕捉能力，有望更好地处理传统方法难以应对的反语、讽刺和混合情感等情况。

## 从测试到应用

这次小规模测试显示，大语言模型在情感分析任务上的准确性（0.95）明显高于传统 NLP 方法（0.775），特别是在避免严重误判方面表现突出。传统方法出现了 15% 的方向性错误（把正向判断为负向或反之），而大模型没有出现这类错误。

不过需要明确的是，这只是基于虚构数据的初步验证。真实的员工反馈往往更复杂，包含更多的反语、暗示和混合情感。要在实际业务中应用，还需要在更大规模、更真实的数据上进行充分测试和调优。但至少这个方向是值得探索的，大模型为理解员工心声提供了一个新的技术选项。
