---
title: 用假设问题提升 RAG 检索准确度
description: 通过多向量检索和假设提问技术，解决 RAG 系统中用户问题与文档内容语义不匹配的检索难题
releaseDate: 2024-05-18
author:
  name: Richard Wang
  url: https://imrichard.com
---

import { Step, Steps } from 'fumadocs-ui/components/steps';

在之前的[财报问答实验](/docs/ai/applications/rag-pdf-qa)中，遇到过一个典型问题：用户问"腾讯2023年收入比2022年高多少"，但向量检索总是召回不到正确的段落。

原因在于表述方式的差异。文档里写的是"截至二零二三年十二月三十一日止年度的收入同比增长10%至人民币6,090亿元"，这种财报式的正式表述和用户口语化的提问方式，在语义空间中的距离并不近。简单的相似度匹配很难把它们关联起来。

解决这个问题的一个有效方法是**多向量检索**。具体来说，就是让 LLM 针对每个文档片段生成几个假设性问题（用户可能会问的问题），然后对这些问题进行向量化。检索时，用户的真实问题会先匹配到这些假设问题，再通过假设问题找到对应的原始文档片段。

这相当于在文档和用户问题之间架了一座桥梁，用 LLM 的理解能力来弥补表述方式的差异。

## 生成假设问题

文档导入和切分的步骤这里不再重复，可以参考之前的[基础 RAG 实现](/docs/ai/applications/rag-pdf-qa)。假设我们已经有了切分好的文档片段列表 `data`，每个元素是一个 `Document` 对象。

<Steps>
<Step>

### 构建 Prompt

首先需要设计一个 Prompt，让 LLM 为每个文档片段生成几个假设问题。这些问题应该是基于文档内容，用户可能会问出来的。

```python
prompt = """
Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:

doc name:
{doc_name}

doc content:
{doc}

Output format (must be a valid Python list):
["腾讯2023年度的财务表现相比于2022年度有何变化？具体体现在哪些关键财务指标上?",
"腾讯2023年度综合业绩报告中，未经重列和经重列的经营盈利、期内盈利以及本公司权益持有人应占盈利分别下降了多少百分比？",
"2023年与2022年相比，董事会如何解释腾讯综合财务报表中出现的大幅盈利下滑，特别是基本每股盈利和摊薄每股盈利的下降幅度？"]
"""
```

这里要求生成 3 个问题，实际使用时可以根据需要调整数量。问题越多，检索覆盖面越广，但也会增加向量存储的开销。

</Step>
<Step>

### 定义生成任务

LLM 返回的是字符串格式的列表，需要转换成真正的 Python 列表：

```python
# 将模型输出的字符串转换为列表
def ListOutputParser(input):
    import ast
    return ast.literal_eval(input.content)

from langchain_core.prompts import ChatPromptTemplate
```

定义 LangChain 任务链。输入是 `Document` 对象，所以用 lambda 函数提取文档名称和内容：

```python
# 定义任务 Chain
chain = (
    {"doc_name": lambda x: x.metadata["source"], "doc": lambda x: x.page_content}
    | ChatPromptTemplate.from_template(prompt)
    | model
    | ListOutputParser
)
```

</Step>
<Step>

### 单个文档的生成效果

先看一个文档片段的生成效果：

```python
hypothetical_questions = chain.invoke(data[0])
```

输出示例：

```python
['腾讯2023年度的财务表现相比于2022年度的具体增长或下降情况在哪些方面最为显著？',
 '腾讯2023年非国际财务报告准则下的经营盈利和权益持有人应占盈利相比于上一年度分别增长了多少，这种变化的主要驱动因素是什么？',
 '2023年财务报告中提到的重分类项目对经营盈利和每股盈利的影响如何，以及这种调整如何影响了与2022年的比较结果？']
```

可以看到，LLM 能够理解财报的内容，并生成相对自然的问题形式。

</Step>
<Step>

### 批量处理所有文档

实际应用中需要为所有文档片段生成假设问题，可以用批量并行处理来提速：

```python
hypothetical_questions = chain.batch(data, {"max_concurrency": 5})
```

<Callout type="info">
  `max_concurrency` 参数控制并发数。设置为 5 表示同时处理 5
  个文档片段。具体数值需要根据你的 API
  限额和硬件资源调整。太大可能触发限流，太小会拖慢处理速度。
</Callout>

输出是一个双层列表，每个文档片段对应一个包含 3 个问题的子列表：

```python
[['腾讯2023年度的财务表现相比于2022年度的具体增长或下降情况在哪些方面最为显著？',
  '腾讯2023年非国际财务报告准则下的经营盈利和权益持有人应占盈利相比于上一年度分别增长了多少，这种变化的主要驱动因素是什么？',
  '2023年财务报告中提到的若干项目重新分类对经营盈利和每股盈利的影响如何，以及这种调整如何影响了与2022年的比较数据？'],
 ['腾讯2023年第四季度及全年业绩报告中，各项财务数据的具体同比增长了多少，哪些部分驱动了总收入和毛利的增长或下降？',
  '在非国际财务报告准则下，腾讯2023年的经营盈利和权益持有人应占盈利相比于2022年分别增长了多少，这种增长的主要原因是什么？',
  '腾讯在2023年度的业绩下滑中，尤其是基本每股盈利和摊薄每股盈利的大幅下降，公司如何在报告附注中解释这些重列项目对净利润的影响以及可能的战略调整？'],
 ['腾讯2023年第四季度及全年业绩中，股息增长了多少，以及其批准和派发的时间点是什么？',
  '在2023年，微信、QQ和收费增值服务的月活跃账户数分别有何变化，这反映了哪些业务板块的增减趋势？',
  '腾讯在2023年的业务回顾中提到了哪些关键产品或服务的亮点，以及这些亮点如何影响了公司的毛利和资本回报计划？'],
 # ... 更多文档片段的假设问题
]
```

</Step>
</Steps>

## 构建多向量检索器

生成假设问题后，需要将它们向量化存储，并建立与原始文档片段的关联。LangChain 提供了 `MultiVectorRetriever` 来简化这个过程。

<Steps>
<Step>

### 初始化存储组件

需要两个存储层：

```python
from langchain_community.vectorstores import Chroma
from langchain.storage import InMemoryByteStore

# 向量数据库：存储假设问题的向量
vectorstore = Chroma(
    collection_name="hypo-questions",
    embedding_function=embeddings
)

# 内存存储：存储原始文档片段
store = InMemoryByteStore()
id_key = "doc_id"
```

初始化多向量检索器：

```python
from langchain.retrievers.multi_vector import MultiVectorRetriever

retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    byte_store=store,
    id_key=id_key,
)
```

这个检索器的工作机制是：用户问题 → 匹配假设问题的向量 → 通过 ID 找到对应的原始文档片段 → 返回原始片段给 LLM。

</Step>
<Step>

### 准备数据并建立关联

为每个原始文档片段生成唯一 ID：

```python
import uuid

doc_ids = [str(uuid.uuid4()) for _ in data]
```

将假设问题和 `doc_id` 封装为 `Document` 对象：

```python
question_docs = []

for i, question_list in enumerate(hypothetical_questions):
    question_docs.extend(
        [Document(page_content=s, metadata={id_key: doc_ids[i]})
         for s in question_list]
    )
```

这样，每个假设问题都会带上对应原始文档的 ID。查看封装后的结构：

```python
[Document(page_content='腾讯2023年度的财务表现相比于2022年度的具体增长或下降情况在哪些方面最为显著？',
          metadata={'doc_id': 'c3c212a5-9512-45f0-92d7-fe261289ac12'}),
 Document(page_content='腾讯2023年非国际财务报告准则下的经营盈利和权益持有人应占盈利相比于上一年度分别增长了多少，这种变化的主要驱动因素是什么？',
          metadata={'doc_id': 'c3c212a5-9512-45f0-92d7-fe261289ac12'}),
 Document(page_content='2023年财务报告中提到的若干项目重新分类对经营盈利和每股盈利的影响如何，以及这种调整如何影响了与2022年的比较数据？',
          metadata={'doc_id': 'c3c212a5-9512-45f0-92d7-fe261289ac12'}),
 # ... 更多假设问题
]
```

注意前三个假设问题的 `doc_id` 相同，它们对应同一个原始文档片段。

</Step>
<Step>

### 存储向量和建立映射

将假设问题向量化并存入向量数据库：

```python
retriever.vectorstore.add_documents(question_docs)
```

建立假设问题 ID 与原始文档片段的映射关系：

```python
retriever.docstore.mset(list(zip(doc_ids, data)))
```

现在数据准备完成，可以开始检索了。

</Step>
</Steps>

## 检索效果验证

<Steps>
<Step>

### 召回假设问题

先看向量数据库直接召回的假设问题：

```python
sub_docs = vectorstore.similarity_search("腾讯2023年收入比2022年高多少")
```

召回结果：

```python
[Document(page_content='腾讯2023年第四季度及全年财务数据与2022年相比的具体对比分析？',
          metadata={'doc_id': 'c7a1a26e-aade-4334-b33e-61778138be01'}),
 Document(page_content='腾讯2023年第四季度和全年在金融科技及增值服务、网络广告、企业服务等业务板块的收入和毛利分别增长了多少？',
          metadata={'doc_id': 'eb6afc2e-d97d-453b-b5ee-27da8dca6ba4'}),
 Document(page_content='腾讯2023年第四季度及全年收入增长了多少？增长的主要驱动因素是什么？',
          metadata={'doc_id': '11f65ce4-7e23-40b6-a224-ab3716cff1c9'}),
 Document(page_content='腾讯2023年金融科技及企业服务业务的收入增长了多少，以及这一增长的主要驱动因素是什么？',
          metadata={'doc_id': 'e100e3eb-bafd-43a4-84de-38636d7145c3'})]
```

可以看到，用户的问题"腾讯2023年收入比2022年高多少"成功匹配到了语义相近的假设问题，特别是第三个"腾讯2023年第四季度及全年收入增长了多少？"。

实际应用中，`MultiVectorRetriever` 会自动通过这些假设问题的 `doc_id` 找到对应的原始文档片段，并将原始片段返回给 LLM。

</Step>
<Step>

### 构建问答链

最后一步是构建完整的 RAG 问答流程：

```python
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

template = """
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.

Context: {context}

Question: {question}

Answer:
"""

rag_prompt = PromptTemplate.from_template(template)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | rag_prompt
    | model
    | StrOutputParser()
)
```

这里的 `retriever` 是前面配置好的 `MultiVectorRetriever`，它会自动完成"用户问题 → 假设问题 → 原始文档"的检索链路。

</Step>
<Step>

### 最终效果

测试一下效果：

```python
question = "腾讯2023年收入比2022年高多少？"
answer = rag_chain.invoke(question)

print(f'\nQuestion: {question}\nAnswer: {answer}')
```

输出：

```
Question: 腾讯2023年收入比2022年高多少？
Answer: 腾讯2023年的收入同比增长了10%，从2022年的554.552亿元人民币增长到609.015亿元人民币。
```

准确命中。通过假设问题这个中间层，系统成功召回了包含答案的文档片段，LLM 也给出了准确的回答。

</Step>
</Steps>

## 方法的适用场景

假设问题这个技巧特别适合处理用户问题和文档内容表述方式差异较大的场景，比如：

- **财报、法律文书等正式文档**：文档用词规范、句式复杂，而用户提问往往更口语化。
- **技术文档**：文档侧重描述实现细节，用户更关心"怎么用"、"能解决什么问题"。
- **产品说明书**：文档按功能点组织，用户按问题场景提问。

这个方法的成本主要在生成假设问题时的 LLM 调用，如果文档片段很多（几千上万个），需要考虑批量处理的时间和 API 费用。但一旦生成完成，检索阶段的开销和普通 RAG 基本一致。

对于文档内容和用户问题本来就很接近的场景（如问答对数据集、社区讨论），这个方法的提升可能不明显，甚至会因为多了一层转换而引入噪音。要不要用，还是得看实际测试效果。
