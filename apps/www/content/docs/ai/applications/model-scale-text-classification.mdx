---
title: 模型规模对文本分类性能的影响
description: 通过对比 14B、34B、72B 模型在复杂分类任务上的表现，探讨模型规模对分类准确性的实际影响，并分析关键影响因素
releaseDate: 2024-02-03
author:
  name: Richard Wang
  url: https://imrichard.com
image: https://imrichard.com/wp-content/uploads/2025/04/impact-of-model-scale-on-text-classification-cover.avif
---

大型语言模型（LLM）在文本分类、信息抽取等自动化处理任务中展示了不小的潜力。对于一些基于语义理解的任务，大家可能觉得，参数规模小一些的模型也许就够用了。但如果任务变得更复杂呢？比如，我们需要对员工调研的反馈进行多维度、细粒度的分析，不仅要理解字面意思，还要捕捉潜在情感、识别特定主题、判断是否包含敏感信息。

**这时候，模型需要多大的参数规模，才能比较好地理解细微的语义差别、遵循复杂的指令，并做出多方面的准确判断？** 为了弄清楚模型规模对处理这类复杂分类任务性能的实际影响，我们设计并进行了一项对比实验。本文将重点看看 Qwen-14B、Yi-34B 和 Qwen-72B 这三款不同规模的模型，在同一个我们专门设计的员工反馈分类任务上的具体表现。我们希望通过实际数据，看看模型规模的提升是否真的带来了显著的性能差异。

## 任务设计

为了尽可能贴近实际工作中的分析需求，我们设计了一个多维度的文本分类任务。任务数据基于模拟的员工调研反馈，要求模型从以下四个维度对每一条反馈进行分析和标记：

1. **回复有效性**：判断反馈内容是否有效（例如，区分开只是敷衍的"无"或空白回复）。
2. **敏感信息识别**：判断反馈中是否提及具体人名、部门名，或者涉及对上级的严重管理问题投诉。
3. **回复内容主题**：将反馈内容归类到预设的几个主题中（如：上级沟通、目标管理、年终奖打折等）。
4. **情感倾向**：判断反馈内容表达的情感是正向、中性还是负向。

这个任务的设计，不仅仅考验模型基础的文本理解能力，更要求它能在多个层面上进行细致的语义分析和推理判断。通过这样的设置，我们可以更清楚地观察到不同规模模型在处理复杂自然语言处理（NLP）任务时的能力差异。

### 任务定义：给模型的具体要求

下面是我们为这次员工反馈分类任务设计的提示词：

```markdown
作为一个NLP专家，你需要评估员工敬业度调研中的回复内容。请按照以下步骤操作：

1. **回复有效性判断**：若回复非常短，如一个词、符号或空白，判断为"无效"。超过10个字即为"有效"。
2. **情感分类**：根据回复内容，将情绪归类为"正向"、"中性"或"负向"。注意回复的情感色彩、态度和情绪。对于使用反话或反讽的回复，尝试识别实际意图，并据此分类。
3. **主题分类**：根据回复的具体内容，将其归类到以下类别，并仅输出分类名称。以下是分类名称的表格形式呈现：

    | 分类名称       |
    |--------------------|
    | 上级沟通       |
    | 公开公示       |
    | 标准透明       |
    | 客观公正       |
    | 目标管理       |
    | 系统体验       |
    | 政策制定       |
    | 年终奖打折       |

4. **是否敏感信息**：根据回复内容，判断是否包含敏感信息，并填写"是"或"否"。敏感信息包括：
    *   提到了具体人名或具体部门名称
    *   举报、投诉所在部门的上级管理者的严重管理问题

请基于提供的回复内容做出判断，避免任何推测或脑补。

**要点提醒**：
*   直接回答每个任务的问题。
*   使用明确的词汇"有效"或"无效"来描述回复的有效性。
*   确保情感分类结果仅为"正向"、"中性"或"负向"之一。
*   在主题分类任务中，确保仅返回表中的分类名称。
*   在判断是否包含敏感信息时，使用"是"或"否"来明确回答。

员工ID与员工回复 >>>{员工反馈的具体内容会放在这里}<<<

按以上要求输出结果，不要包含任何额外信息。

{这里会告诉模型期望的输出格式}
```

这份提示词详细规定了模型需要完成的四项分析任务（有效性、情感、主题、敏感信息）、具体的判断标准（如字数、特定内容）、主题的预设范围以及输出格式的要求。我们使用这份完全相同的提示词，来测试 Qwen-14B、Yi-34B 和 Qwen-72B 这三款模型处理同一批员工反馈数据的表现。

## 实验结果与分析

<Callout type="info">
本次实验使用的文本样本由 ChatGPT 辅助生成，其中提到的人名均为虚构。相关生成方法可参考我们之前的文章《利用大模型提升情感分类任务准确性》。
</Callout>

接下来看看各模型在处理这批样本时的实际表现：

### Qwen-14B

Qwen-14B 在处理这个任务时，观察到几个值得注意的现象：

![表格展示Qwen-14B模型处理员工反馈的分类结果，其中可见一些分类错误。](https://imrichard.com/wp-content/uploads/2025/04/qwen-14b-classification-results.avif)

1. **主题分类偏离**：在第一个样本（"感谢公司，感谢老板！"）中，模型将主题分到了"感谢"，但这并不在我们预设的主题列表中。这似乎表明模型在生成回答时，可能未能完全遵循提示词中关于主题分类范围的明确限定，出现了"遗忘"任务要求的情况。
2. **有效性判断偏差**：第二个样本（"无意见"）按我们的规则（少于10字）应为"无效"，但模型判断为"有效"。这显示出模型对指令中细节规则的理解和执行上存在偏差。
3. **敏感信息识别不足**：第四个样本明确提到了人名"刘海涛"并涉及负面反馈，按规则应判断为"是"敏感信息，但模型判断为"否"。

### Yi-34B

Yi-34B 模型相比 Qwen-14B 在某些方面有所进步，但在理解特定中文表达上遇到了困难：

![表格展示Yi-34B模型处理员工反馈的分类结果，其在情感和主题分类上的一些特定理解偏差。](https://imrichard.com/wp-content/uploads/2025/04/yi-34b-classification-results.avif)

- 最明显的问题出现在第三个样本（"希望年终奖不要再打折了"）的情感判断上。这个表达在中国文化背景下通常带有明显的负面或抱怨意味，但模型却将其分类为"正向"。我们尝试与模型进行对话，询问"年终奖打折是什么意思"，发现它不仅无法准确解释，甚至将其理解为某种员工福利。这反映出模型可能缺乏对这类具有特定文化或行业背景词汇的深入理解。

### Qwen-72B

当模型规模扩大到 72B 时，其在处理这个复杂分类任务上的表现有了显著提升，整体结果比较稳健：

![表格展示Qwen-72B模型处理员工反馈的分类结果，显示其在多维度分类任务上表现较为准确和稳定。](https://imrichard.com/wp-content/uploads/2025/04/qwen-72b-classification-results.avif)

- 它准确地将样本2（"无意见"）判断为"无效"，并相应地给出了"无法判断"的情感和空主题列表，这完全符合我们的预期。
- 对于包含特定负面含义词汇的样本3（"希望年终奖不要再打折了"），它正确判断为"负向"。
- 对于包含人名和潜在投诉的样本4和5，它也准确识别出了敏感信息。

## 总结

通过这次直接对比，我们看到，对于我们设计的这个包含多维度判断、需要一定理解和推理能力的文本分类任务来说，**模型规模与分类的准确性和稳定性之间，确实存在比较明显的关系**。规模更大的模型（Qwen-72B）在整体表现上优于规模较小的模型（Qwen-14B 和 Yi-34B）。

具体来看，影响准确性的几个因素似乎与模型规模有关：

1. **指令遵循与"遗忘"现象**：Qwen-14B 出现了将主题分到预设列表之外的情况，这可能是模型在处理较长或较复杂的指令时，未能始终"记住"所有约束条件。通常，模型规模越大，其上下文理解和记忆能力会相应增强，出现这类"遗忘"的几率可能会降低。
2. **领域知识与语义理解深度**：Yi-34B 对"年终奖打折"这类特定表达的理解偏差，反映了模型内置知识库可能存在的局限性。更大规模的模型往往在更广泛的数据上进行训练，其"原生"知识储备更丰富，对各种表达方式（包括一些俚语、行业术语或隐含意义）的理解可能更到位。
3. **综合推理与判断能力**：整个任务需要模型结合多个维度的规则进行判断。Qwen-72B 在有效性判断、情感识别（尤其是在有潜在干扰词的情况下）和敏感信息识别等多个方面都展现出更强的综合处理能力。这通常被认为是更大模型更强推理能力的体现。
4. **硬件资源考量**：当然，一个不可忽视的现实是，更大规模的模型意味着更高的硬件资源需求（显存、计算力）。在实际部署应用时，这必然是一个需要权衡的因素。

那么，回到最初的问题：要确保模型能较好地处理这类复杂分类任务，需要多大规模？从我们的实验来看，**如果任务复杂性高，且对准确性和稳定性有较高要求，选用更大规模的模型往往能提供更可靠的基础性能。** 它们在理解细微语义、遵循复杂指令和利用背景知识方面，确实表现出更扎实的能力。

**但需要强调的是**，这次实验主要是在相对一致和基础的提示词（Prompt）条件下进行的，目的是观察模型规模本身带来的能力差异。

在实际项目中，我们**完全可以通过更精细化的提示工程来提升模型表现**，尤其是对于中小型规模的模型。例如：

- **强化指令**：在提示词中更明确、甚至重复地强调关键规则和分类边界。
- **补充知识**：如果发现模型缺乏某些关键概念的理解（比如"年终奖打折"），可以在提示词中直接给出定义或解释。
- **少量示例学习 (Few-shot Learning)**：在提示词里提供几个正确分类的完整示例，让模型"照猫画虎"。
- **任务分解**：将复杂的多维度分类任务拆解成几个更简单的单维度分类子任务，分步执行。

通过这些优化手段，即使是 14B 规模的模型，经过针对性的调优，也完全有可能在我们这个任务上取得相当不错的效果。

因此，最终的选择，需要在**预期的效果、可接受的开发成本（调优所需的时间和精力）以及可用的运行资源**之间找到一个务实的平衡点。本次对比的核心观察——在同等或较少优化的情况下，模型规模是影响其处理复杂任务能力上限的一个关键因素——希望能为大家在模型选型时提供一个有价值的参考。

