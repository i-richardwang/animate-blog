---
title: 模型规模对文本分类性能的影响
description: 通过对比 14B、34B、72B 模型在复杂分类任务上的表现，探讨模型规模对分类准确性的实际影响，并分析关键影响因素
releaseDate: 2024-02-03
author:
  name: Richard Wang
  url: https://imrichard.com
---

大型语言模型（LLM）在文本分类、信息抽取等自动化处理任务中展示了不小的潜力。对于一些基于语义理解的任务，大家可能觉得，参数规模小一些的模型也许就够用了。但如果任务变得更复杂呢？比如，我们需要对员工调研的反馈进行多维度、细粒度的分析，不仅要理解字面意思，还要捕捉潜在情感、识别特定主题、判断是否包含敏感信息。

**这时候，模型需要多大的参数规模，才能比较好地理解细微的语义差别、遵循复杂的指令，并做出多方面的准确判断？** 为了弄清楚模型规模对处理这类复杂分类任务性能的实际影响，我们设计并进行了一项对比实验。本文将重点看看 Qwen-14B、Yi-34B 和 Qwen-72B 这三款不同规模的模型，在同一个我们专门设计的员工反馈分类任务上的具体表现。我们希望通过实际数据，看看模型规模的提升是否真的带来了显著的性能差异。

## 任务设计

为了尽可能贴近实际工作中的分析需求，我们设计了一个多维度的文本分类任务。任务数据基于模拟的员工调研反馈，要求模型从以下四个维度对每一条反馈进行分析和标记：

1. **回复有效性**：判断反馈内容是否有效（例如，区分开只是敷衍的"无"或空白回复）。
2. **敏感信息识别**：判断反馈中是否提及具体人名、部门名，或者涉及对上级的严重管理问题投诉。
3. **回复内容主题**：将反馈内容归类到预设的几个主题中（如：上级沟通、目标管理、年终奖打折等）。
4. **情感倾向**：判断反馈内容表达的情感是正向、中性还是负向。

这个任务的设计，不仅仅考验模型基础的文本理解能力，更要求它能在多个层面上进行细致的语义分析和推理判断。通过这样的设置，我们可以更清楚地观察到不同规模模型在处理复杂自然语言处理（NLP）任务时的能力差异。

### 任务定义：给模型的具体要求

下面是我们为这次员工反馈分类任务设计的提示词：

```markdown
作为一个NLP专家，你需要评估员工敬业度调研中的回复内容。请按照以下步骤操作：

1. **回复有效性判断**：若回复非常短，如一个词、符号或空白，判断为"无效"。超过10个字即为"有效"。
2. **情感分类**：根据回复内容，将情绪归类为"正向"、"中性"或"负向"。注意回复的情感色彩、态度和情绪。对于使用反话或反讽的回复，尝试识别实际意图，并据此分类。
3. **主题分类**：根据回复的具体内容，将其归类到以下类别，并仅输出分类名称。以下是分类名称的表格形式呈现：

   | 分类名称   |
   | ---------- |
   | 上级沟通   |
   | 公开公示   |
   | 标准透明   |
   | 客观公正   |
   | 目标管理   |
   | 系统体验   |
   | 政策制定   |
   | 年终奖打折 |

4. **是否敏感信息**：根据回复内容，判断是否包含敏感信息，并填写"是"或"否"。敏感信息包括：
   - 提到了具体人名或具体部门名称
   - 举报、投诉所在部门的上级管理者的严重管理问题

请基于提供的回复内容做出判断，避免任何推测或脑补。

**要点提醒**：

- 直接回答每个任务的问题。
- 使用明确的词汇"有效"或"无效"来描述回复的有效性。
- 确保情感分类结果仅为"正向"、"中性"或"负向"之一。
- 在主题分类任务中，确保仅返回表中的分类名称。
- 在判断是否包含敏感信息时，使用"是"或"否"来明确回答。

员工ID与员工回复 >>>{员工反馈的具体内容会放在这里}<<<

按以上要求输出结果，不要包含任何额外信息。

{这里会告诉模型期望的输出格式}
```

这份提示词详细规定了模型需要完成的四项分析任务（有效性、情感、主题、敏感信息）、具体的判断标准（如字数、特定内容）、主题的预设范围以及输出格式的要求。我们使用这份完全相同的提示词，来测试 Qwen-14B、Yi-34B 和 Qwen-72B 这三款模型处理同一批员工反馈数据的表现。

## 实验结果与分析

<Callout type="info">
  本次实验使用的文本样本由 ChatGPT
  辅助生成，其中提到的人名均为虚构。相关生成方法可参考我们之前的文章《利用大模型提升情感分类任务准确性》。
</Callout>

接下来看看各模型在处理这批样本时的实际表现：

### Qwen-14B

Qwen-14B 在处理这个任务时，观察到几个值得注意的现象：

![表格展示Qwen-14B模型处理员工反馈的分类结果，其中可见一些分类错误。](/ai/applications/qwen-14b-classification-results.webp)

1. **主题分类偏离**：在第一个样本（"感谢公司，感谢老板！"）中，模型将主题分到了"感谢"，但这并不在我们预设的主题列表中。这似乎表明模型在生成回答时，可能未能完全遵循提示词中关于主题分类范围的明确限定，出现了"遗忘"任务要求的情况。
2. **有效性判断偏差**：第二个样本（"无意见"）按我们的规则（少于10字）应为"无效"，但模型判断为"有效"。这显示出模型对指令中细节规则的理解和执行上存在偏差。
3. **敏感信息识别不足**：第四个样本明确提到了人名"刘海涛"并涉及负面反馈，按规则应判断为"是"敏感信息，但模型判断为"否"。

### Yi-34B

Yi-34B 模型相比 Qwen-14B 在某些方面有所进步，但在理解特定中文表达上遇到了困难：

![表格展示Yi-34B模型处理员工反馈的分类结果，其在情感和主题分类上的一些特定理解偏差。](/ai/applications/yi-34b-classification-results.webp)

- 最明显的问题出现在第三个样本（"希望年终奖不要再打折了"）的情感判断上。这个表达在中国文化背景下通常带有明显的负面或抱怨意味，但模型却将其分类为"正向"。我们尝试与模型进行对话，询问"年终奖打折是什么意思"，发现它不仅无法准确解释，甚至将其理解为某种员工福利。这反映出模型可能缺乏对这类具有特定文化或行业背景词汇的深入理解。

### Qwen-72B

当模型规模扩大到 72B 时，其在处理这个复杂分类任务上的表现有了显著提升，整体结果比较稳健：

![表格展示Qwen-72B模型处理员工反馈的分类结果，显示其在多维度分类任务上表现较为准确和稳定。](/ai/applications/qwen-72b-classification-results.webp)

- 它准确地将样本2（"无意见"）判断为"无效"，并相应地给出了"无法判断"的情感和空主题列表，这完全符合我们的预期。
- 对于包含特定负面含义词汇的样本3（"希望年终奖不要再打折了"），它正确判断为"负向"。
- 对于包含人名和潜在投诉的样本4和5，它也准确识别出了敏感信息。

## 规模很重要，但不是全部

这次对比揭示了一个关键发现：在处理复杂的多维度文本分类任务时，模型规模确实会影响基础性能的上限。Qwen-72B 在指令遵循、语义理解和综合推理方面都表现得更稳定，而 Qwen-14B 会"忘记"约束条件，Yi-34B 则对"年终奖打折"这类表达理解有偏差。

但这不意味着小模型就不能用。本次实验中，所有模型使用的都是相对基础的 Prompt，没有针对性优化。实际项目中，完全可以通过提示工程来提升中小型模型的表现：

- 在 Prompt 中明确重复关键规则，避免"遗忘"
- 补充模型缺失的领域知识（如直接解释"年终奖打折"的含义）
- 提供 Few-shot 示例，让模型"照猫画虎"
- 将复杂任务拆解成多个简单的子任务

经过这些优化，14B 模型也可能达到不错的效果。所以最终的选择是个权衡：如果你的硬件资源充足，想要更稳定的"开箱即用"体验，选大模型；如果资源有限但愿意投入调优时间，小模型也能满足需求。关键是要清楚自己的场景需要什么，能接受什么样的开发成本。
