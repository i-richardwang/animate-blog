---
title: Open Deep Research：深度调研 Agent 的架构与实现
description: 介绍我们基于 LangGraph 构建的开源深度调研 Agent，支持自带模型、搜索工具和 MCP 服务器，生成结构化的深度研究报告
date: 2024-07-16
author:
  name: LangChain Team
  url: https://blog.langchain.com
originalUrl: https://blog.langchain.com/open-deep-research/
image: https://blog.langchain.com/content/images/2025/07/Open-Deep-Research-1.webp
---

### TL;DR

深度调研（Deep Research）已成为最流行的 Agent 应用之一。[OpenAI](https://openai.com/index/introducing-deep-research/?ref=blog.langchain.com)、[Anthropic](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com)、[Perplexity](https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research?ref=blog.langchain.com) 和 [Google](https://gemini.google/overview/deep-research/?hl=en&ref=blog.langchain.com) 都有深度调研产品，这些产品使用[各种](https://www.anthropic.com/news/research?ref=blog.langchain.com)上下文来源生成综合报告。还有许多[开源](https://huggingface.co/blog/open-deep-research?ref=blog.langchain.com) [实现](https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart?ref=blog.langchain.com)。

我们构建了一个[开源深度研究员](https://github.com/langchain-ai/open_deep_research?ref=blog.langchain.com)，它简单且可配置，允许用户带上自己的模型、搜索工具和 MCP 服务器。

- Open Deep Research 基于 LangGraph 构建。在[这里](https://github.com/langchain-ai/open_deep_research?ref=blog.langchain.com)查看代码！
- 在 [Open Agent Platform](https://oap.langchain.com/?ref=blog.langchain.com) 上试用

![Open Deep Research 概览](https://blog.langchain.com/content/images/2025/07/overview.png)

### 挑战

研究是一项开放式任务；回答用户请求的最佳策略无法轻易预知。请求可能需要不同的研究策略和不同程度的搜索深度。

> *"比较这两个产品"*

比较通常受益于对每个产品进行搜索，然后是一个综合步骤来比较它们。

> *"找出这个职位的前 20 名候选人"*

列出和排名请求通常需要开放式搜索，然后是综合和排名。

> *"X 真的是真的吗？"*

验证问题可能需要对特定领域进行迭代深入研究，其中来源的质量比搜索的广度更重要。

考虑到这些点，开源深度研究的一个关键设计原则是**灵活性**，以便根据请求探索不同的研究策略。

### 架构概览

[Agent](https://langchain-ai.github.io/langgraph/tutorials/workflows/?ref=blog.langchain.com#agent) 非常适合研究，因为它们可以灵活应用不同的策略，利用中间结果来指导探索。Open Deep Research 使用一个 Agent 来进行研究，作为三步过程的一部分：

- **界定范围 (Scope)** – *阐明研究范围*
- **开展研究 (Research)** – *执行研究活动*
- **撰写报告 (Write)** – *生成最终报告*

![三阶段架构](https://blog.langchain.com/content/images/2025/07/simple.png)

#### 阶段 1：界定范围 (Scope)

界定范围的目的是收集研究所需的所有用户上下文。这是一个两步流程，执行**用户澄清**和**简报生成**。

![Scope 阶段流程](https://blog.langchain.com/content/images/2025/07/scope.png)

**用户澄清**

[OpenAI 指出](https://youtu.be/bNEvJYzoa8A?feature=shared&t=1265&ref=blog.langchain.com)，用户很少在研究请求中提供足够的上下文。我们在必要时使用聊天模型询问额外的上下文。

![用户澄清交互](https://blog.langchain.com/content/images/2025/07/brief.png)

**简报生成**

聊天交互可能包括澄清问题、后续行动或用户提供的示例（例如，之前的深度研究报告）。因为交互可能非常冗长且 Token 密集，我们将它转化为一份全面而集中的研究简报。研究简报作为我们成功的北极星指标，我们在整个研究和写作阶段都会参考它。

![研究简报示例](https://blog.langchain.com/content/images/2025/07/actualbrief.png)

💡

*我们将研究员与用户的聊天交互转化为一个集中的简报，供研究监督者衡量。*

#### 阶段 2：开展研究 (Research)

研究的目标是收集研究简报要求的上下文。我们使用一个[监督者 Agent](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/?ref=blog.langchain.com) 进行研究。

![Research 阶段架构](https://blog.langchain.com/content/images/2025/07/research.png)

**研究监督者 (Research Supervisor)**

监督者的工作很简单：将研究任务委派给适当数量的子 Agent。监督者确定研究简报是否可以分解为独立的子主题，并委派给具有隔离上下文窗口的子 Agent。这很有用，因为它允许系统并行化研究工作，更快地找到更多信息。

**研究子 Agent (Research Sub-Agents)**

每个研究子 Agent 都会从监督者那里收到一个子主题。子 Agent 被提示只关注特定主题，而不必担心研究简报的完整范围——那是监督者的工作。每个子 Agent 通过工具调用循环进行研究，利用用户配置的搜索工具和/或 MCP 工具。

当每个子 Agent 完成时，它会进行最后一次 LLM 调用，对其提出的子问题写出详细的答案，考虑到它的所有研究并引用有用的来源。这很重要，因为从工具调用反馈中收集到的信息可能包含大量原始（例如抓取的网页）和不相关（例如失败的工具调用或不相关的网站）信息。

💡

*我们进行额外的 LLM 调用来清理子 Agent 的研究发现，以便为监督者提供干净、经过处理的信息。*

如果我们把这些原始信息返回给监督者，Token 使用量可能会显著膨胀，监督者不得不解析更多的 Token 以隔离最有用的信息。因此，我们的子 Agent 会清理其发现并将其返回给监督者。

![研究监督迭代流程](https://blog.langchain.com/content/images/2025/07/big-research.png)

**研究监督者迭代**

监督者推理子 Agent 的发现是否充分解决了简报中的工作范围。如果监督者想要更深入，它可以生成更多的子 Agent 来进行更多的研究。随着监督者委派研究并反思结果，它可以灵活地识别缺少的内容并通过后续研究填补这些空白。

#### 阶段 3：撰写报告 (Write)

报告撰写的目标是使用从子 Agent 收集的上下文来满足研究简报的请求。当监督者认为收集到的发现足以满足研究简报中的请求时，我们就继续撰写报告。

为了撰写报告，我们向 LLM 提供研究简报和子 Agent 返回的所有研究发现。这最后的 LLM 调用一次性生成输出，由简报引导并用研究发现回答。

![报告写作阶段](https://blog.langchain.com/content/images/2025/07/one-shot.png)

### 经验教训

#### **仅对易于并行化的任务使用多 Agent**

多 Agent 与单 Agent 是一个重要的设计考虑因素。Cognition [反对多 Agent](https://cognition.ai/blog/dont-build-multi-agents?ref=blog.langchain.com)，因为并行工作的子 Agent 可能难以协调。如果任务（例如构建应用程序）需要多 Agent 输出协同工作，那么协调就是一个风险。

我们也吸取了这个教训。我们研究 Agent 的早期版本与子 Agent 并行编写最终报告的章节。虽然速度很快，但我们遇到了 Cognition 提出的问题：报告是不连贯的，因为编写章节的 Agent 协调得不好。我们通过仅将多 Agent 用于研究任务本身，在所有研究完成后进行写作，解决了这个问题。

💡

*多 Agent 难以协调，如果并行编写报告章节可能会表现不佳。我们将多 Agent 限制在研究阶段，并一次性撰写报告。*

#### 多 Agent 能够隔离子研究主题的上下文

我们的实验表明，如果请求包含多个子主题（例如，比较 A、B 和 C），单 Agent 的响应质量会受到影响。直觉很简单：单个上下文窗口需要存储和推理所有子主题的工具反馈。此工具反馈通常是 Token 密集的。随着上下文窗口积累跨许多不同子主题的工具调用，[无数的失败模式](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com)，例如[上下文冲突](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-clash)，变得普遍。

**让我们看一个具体例子**

*比较 OpenAI、Anthropic 和 Google DeepMind 在 AI 安全方面的方法。我想了解它们不同的哲学框架、研究优先级以及它们如何思考对齐问题。*

我们的 **单 Agent** 实现使用其搜索工具*同时*发送关于每个前沿实验室的单独查询。

- 'OpenAI AI 安全和对齐的哲学框架'
- 'Anthropic AI 安全和对齐的哲学框架'
- 'Google DeepMind AI 安全和对齐的哲学框架’

搜索工具在一个*单一的冗长字符串*中返回了关于所有三个实验室的结果。我们的单 Agent 推理了所有三个前沿实验室的结果，并再次调用搜索工具，询问每个实验室的独立查询。

- 'DeepMind 关于社会选择和政治哲学的声明'
- 'Anthropic 关于技术对齐挑战的声明'
- 'OpenAI 关于递归奖励建模的技术报告'

在每次工具调用迭代中，单 Agent 都要处理来自三个独立线程的上下文。从 Token 和延迟的角度来看，这很浪费。我们不需要关于 OpenAI 递归奖励建模方法的 Token 来帮助我们生成关于 DeepMind 对齐哲学的下一个查询。另一个重要的观察是，处理多个主题的单 Agent 在选择完成之前，自然会对每个主题进行*较浅*（搜索查询数量较少）的研究。

![多 Agent vs 单 Agent 对比](https://blog.langchain.com/content/images/2025/07/multi-agent.png)

多 Agent 方法允许多个子 Agent 并行运行，每个子 Agent 都致力于一个独立的、专注的任务。将多 Agent 方法应用于研究捕获了 [Anthropic 报告](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com)的益处，并在我们要自己的评估中得到了强调：子主题上下文可以在每个子 Agent 中被隔离。

💡

*在研究期间隔离子主题的上下文可以避免各种长上下文失败模式。*

#### **多 Agent 监督者使系统能够调整所需的研究深度**

用户不希望简单的请求花费 10 分钟以上。但是，有一些请求需要具有更高 Token 利用率和延迟的研究，[正如 Anthropic 很好地展示的那样](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com)。

监督者可以通过选择性地生成子 Agent 来处理这两种情况，以调整请求所需的研究深度水平。监督者被提示使用启发式方法来推理何时应该并行化研究，以及何时单线程研究就足够了。我们的深度研究 Agent 可以灵活地选择是否并行化研究。

💡

*多 Agent 监督者允许搜索策略的灵活性。*

#### 上下文工程对于减轻 Token 膨胀和引导行为很重要

研究是一项 Token 密集的任务。Anthropic 报告说，他们的多 Agent 系统[使用的 Token 是典型聊天应用程序的 15 倍](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com)！我们使用[上下文工程](https://blog.langchain.com/context-engineering-for-agents/)来缓解这个问题。

我们将聊天记录压缩成一份研究简报，这防止了先前消息导致的 Token 膨胀。子 Agent 在返回给监督者之前修剪其研究发现，以删除不相关的 Token 和信息。

如果没有足够的上下文工程，我们的 Agent 很容易因为冗长的原始工具调用结果而遇到上下文窗口限制。实际上，这也有助于节省 Token 支出，并有助于避免触及模型速率限制 (TPM)。

💡

*上下文工程有许多实际好处。它节省 Token，帮助避免上下文窗口限制，并帮助保持在模型速率限制之下。*

### 下一步

Open Deep Research 是一个活跃的项目，我们有一些想法想要尝试。这些是我们正在思考的一些开放性问题。

- 处理 Token 密集型工具响应的最佳方法是什么？过滤掉不相关上下文以减少不必要 Token 支出的最佳方法是什么？
- 是否值得在 Agent 的热路径中运行任何评估以确保高质量的响应？
- 深度研究报告很有价值且创建成本相对较高，我们能否存储这项工作并在未来利用长期记忆来利用它们？

### 使用 Open Deep Research

#### LangGraph Studio

你可以克隆我们的 LangGraph [代码](https://github.com/langchain-ai/open_deep_research?ref=blog.langchain.com)并使用 LangGraph Studio 本地运行 Open Deep Research。你可以使用 Studio 测试提示词和架构，并针对你的用例进行更具体的定制！

[查看代码仓库！](https://github.com/langchain-ai/open_deep_research?ref=blog.langchain.com)

#### Open Agent Platform

我们已将 Open Deep Research 托管在我们的 Open Agent Platform (OAP) 演示实例上。OAP 是一个公民开发者平台，允许用户构建、原型设计和使用 Agent——你所要做的就是传入你的 API 密钥。你还可以部署自己的 [OAP](https://docs.oap.langchain.com/?ref=blog.langchain.com) 实例，以与其他 LangGraph Agent 一起托管 Deep Research！

[在 Open Agent Platform 上试用！](https://oap.langchain.com/?ref=blog.langchain.com)
